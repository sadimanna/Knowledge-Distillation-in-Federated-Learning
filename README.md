# Knowledge Distillation in Federated Learning

| Paper Name                                                                                                       | Link                                                                                                         |
|------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| FedSKD: Aggregation-free Model-heterogeneous Federated Learning using Multi-dimensional Similarity Knowledge Distillation | <a href="https://arxiv.org/pdf/2503.18981" class="underline" target="_blank">arXiv 2503.18981</a>                                                         |
| [IEEEXplore] Poisoning Attacks to Knowledge Distillation-Based Federated Learning Under Robust Aggregation Rules | <a href="https://ieeexplore.ieee.org/document/11072238" class="underline" target="_blank">ieeexplore.ieee.org/document/11072238</a>                       |
| Logit Poisoning Attack in Distillation-based Federated Learning and its Countermeasures (HTML)                  | <a href="https://arxiv.org/html/2401.17746v1" class="underline" target="_blank">arXiv 2401.17746v1</a>                                                     |
| On the Byzantine-Resilience of Distillation-Based Federated Learning                                             | <a href="https://openreview.net/forum?id=of6EuHT7de" class="underline" target="_blank">OpenReview</a>                                                      |
| Knowledge-Distillation based Personalized Federated Learning with Distribution Constraints                       | <a href="https://www.sciencedirect.com/science/article/pii/S0893608025008329" class="underline" target="_blank">ScienceDirect</a>                         |
| Resource-Aware Federated Self-Supervised Learning with Global Class Representations                             | <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/13707aad517ddd6c09ea02e0f55e1e7a-Paper-Conference.pdf" class="undefined" target="_blank">NeurIPS 2024</a>|
| Knowledge Distillation in Federated Learning: A Practical Guide                                                 | <a href="https://www.ijcai.org/proceedings/2024/0905.pdf" class="underline" target="_blank">IJCAI 2024 Paper 905</a>                                      |
| FedGMKD: An Efficient Prototype Federated Learning Framework through Knowledge Distillation and Discrepancy-Aware Aggregation | <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/d6520fa7f71dc8e09ed5939a60a64218-Abstract-Conference.html" class="underline" target="_blank">NeurIPS 2024</a>|
| FedSSA: Federated Self-Supervised Aggregation via Global Class Representation                                   | <a href="https://www.ijcai.org/proceedings/2024/0594.pdf" class="underline" target="_blank">IJCAI 2024 Paper 594</a>                                      |
| FedADKD: Adaptive Knowledge Distillation for Robust Federated Learning                                          | <a href="https://dl.acm.org/doi/abs/10.1007/s10044-024-01350-4" class="underline" target="_blank">ACM/Pattern Recognition Letters</a>                     |
| DaFKD: Domain-Aware Federated Knowledge Distillation                                                            | <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DaFKD_Domain-Aware_Federated_Knowledge_Distillation_CVPR_2023_paper.pdf" class="underline" target="_blank">CVPR 2023</a> |
| CoFD: Communication-Efficient and Privacy-Preserving Collaborative Federated Distillation                       | <a href="https://link.springer.com/chapter/10.1007/978-981-96-6954-7_13" class="underline" target="_blank">Springer Chapter</a>                           |
| FedEED: Federated Ensemble and Ensemble Distillation                                                            | <a href="https://openreview.net/pdf?id=fCbTxKYJovs" class="underline" target="_blank">OpenReview</a>                                                       |
| Communication-Efficient Federated Knowledge Distillation with Unlabeled Data                                    |<a href="https://www.nature.com/articles/s41467-022-29763-x" class="undefined" target="_blank">Nature</a>|
| Mutual Knowledge-Distillation-Based Federated Learning for Short-Term Forecasting in Electric IoT Systems | <a href="https://ieeexplore.ieee.org/document/10563995" class="underline" target="_blank">[IEEE Xplore]</a>                                                  |
| Knowledge Distillation in Federated Learning: A Survey                                                          | <a href="https://link.springer.com/article/10.1007/s10791-025-09657-4" class="underline" target="_blank">Springer</a>                                     |
